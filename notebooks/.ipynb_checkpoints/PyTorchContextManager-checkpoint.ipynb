{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0fb81c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 16.51222938299179\n",
      "Epoch 2/100, Loss: 8.802474409341812\n",
      "Epoch 3/100, Loss: 4.718717485666275\n",
      "Epoch 4/100, Loss: 2.531962350010872\n",
      "Epoch 5/100, Loss: 1.349761012941599\n",
      "Epoch 6/100, Loss: 0.7370893098413944\n",
      "Epoch 7/100, Loss: 0.39563882537186146\n",
      "Epoch 8/100, Loss: 0.21602031961083412\n",
      "Epoch 9/100, Loss: 0.12061166437342763\n",
      "Epoch 10/100, Loss: 0.06940885726362467\n",
      "Epoch 11/100, Loss: 0.041451663011685014\n",
      "Epoch 12/100, Loss: 0.026887650368735194\n",
      "Epoch 13/100, Loss: 0.01898988732136786\n",
      "Epoch 14/100, Loss: 0.014736354642082006\n",
      "Epoch 15/100, Loss: 0.012480672739911824\n",
      "Epoch 16/100, Loss: 0.011097295093350112\n",
      "Epoch 17/100, Loss: 0.01048981785424985\n",
      "Epoch 18/100, Loss: 0.010091201576869935\n",
      "Epoch 19/100, Loss: 0.009971328428946435\n",
      "Epoch 20/100, Loss: 0.00988243252504617\n",
      "Epoch 21/100, Loss: 0.00976694407290779\n",
      "Epoch 22/100, Loss: 0.009832109150011092\n",
      "Epoch 23/100, Loss: 0.009742202062625438\n",
      "Epoch 24/100, Loss: 0.009734092251164839\n",
      "Epoch 25/100, Loss: 0.009775442827958614\n",
      "Epoch 26/100, Loss: 0.009792548313271254\n",
      "Epoch 27/100, Loss: 0.009776338731171563\n",
      "Epoch 28/100, Loss: 0.009763338370248675\n",
      "Epoch 29/100, Loss: 0.009743816452100873\n",
      "Epoch 30/100, Loss: 0.009746822499437258\n",
      "Epoch 31/100, Loss: 0.00969500993960537\n",
      "Epoch 32/100, Loss: 0.009756141807883978\n",
      "Epoch 33/100, Loss: 0.009689946047728881\n",
      "Epoch 34/100, Loss: 0.009667380363680422\n",
      "Epoch 35/100, Loss: 0.00974969391245395\n",
      "Epoch 36/100, Loss: 0.009747520234668627\n",
      "Epoch 37/100, Loss: 0.009731990867294371\n",
      "Epoch 38/100, Loss: 0.009804049448575824\n",
      "Epoch 39/100, Loss: 0.009658609807956964\n",
      "Epoch 40/100, Loss: 0.009646124206483364\n",
      "Epoch 41/100, Loss: 0.009746940049808472\n",
      "Epoch 42/100, Loss: 0.009771534882020205\n",
      "Epoch 43/100, Loss: 0.009748083131853491\n",
      "Epoch 44/100, Loss: 0.009680018207291141\n",
      "Epoch 45/100, Loss: 0.009619698103051633\n",
      "Epoch 46/100, Loss: 0.009758425294421613\n",
      "Epoch 47/100, Loss: 0.009800177998840809\n",
      "Epoch 48/100, Loss: 0.009710740152513608\n",
      "Epoch 49/100, Loss: 0.009690039500128478\n",
      "Epoch 50/100, Loss: 0.009696584951598197\n",
      "Epoch 51/100, Loss: 0.00977056025294587\n",
      "Epoch 52/100, Loss: 0.009686193458037451\n",
      "Epoch 53/100, Loss: 0.009637707320507616\n",
      "Epoch 54/100, Loss: 0.009788735449546948\n",
      "Epoch 55/100, Loss: 0.009676523419329897\n",
      "Epoch 56/100, Loss: 0.009830445138504729\n",
      "Epoch 57/100, Loss: 0.009706958488095552\n",
      "Epoch 58/100, Loss: 0.009710839251056314\n",
      "Epoch 59/100, Loss: 0.009721583890495822\n",
      "Epoch 60/100, Loss: 0.009781535307411104\n",
      "Epoch 61/100, Loss: 0.009665845282142982\n",
      "Epoch 62/100, Loss: 0.009735060593811795\n",
      "Epoch 63/100, Loss: 0.009649941115640104\n",
      "Epoch 64/100, Loss: 0.009693662519566715\n",
      "Epoch 65/100, Loss: 0.009705065574962646\n",
      "Epoch 66/100, Loss: 0.009765814611455426\n",
      "Epoch 67/100, Loss: 0.009724508767249063\n",
      "Epoch 68/100, Loss: 0.009677853697212413\n",
      "Epoch 69/100, Loss: 0.009704430704005063\n",
      "Epoch 70/100, Loss: 0.009714820916997269\n",
      "Epoch 71/100, Loss: 0.009800042666029185\n",
      "Epoch 72/100, Loss: 0.009825950051890686\n",
      "Epoch 73/100, Loss: 0.009732635517138988\n",
      "Epoch 74/100, Loss: 0.009627405350329354\n",
      "Epoch 75/100, Loss: 0.009726019285153598\n",
      "Epoch 76/100, Loss: 0.009859780519036576\n",
      "Epoch 77/100, Loss: 0.00979859399376437\n",
      "Epoch 78/100, Loss: 0.009857273049419746\n",
      "Epoch 79/100, Loss: 0.009660226496635005\n",
      "Epoch 80/100, Loss: 0.009679783863248304\n",
      "Epoch 81/100, Loss: 0.009734709310578182\n",
      "Epoch 82/100, Loss: 0.009730062796734273\n",
      "Epoch 83/100, Loss: 0.009774581791134551\n",
      "Epoch 84/100, Loss: 0.009710337151773274\n",
      "Epoch 85/100, Loss: 0.00973063288256526\n",
      "Epoch 86/100, Loss: 0.009674253815319389\n",
      "Epoch 87/100, Loss: 0.009808984294068068\n",
      "Epoch 88/100, Loss: 0.009725770156364888\n",
      "Epoch 89/100, Loss: 0.00969034552690573\n",
      "Epoch 90/100, Loss: 0.009718965244246647\n",
      "Epoch 91/100, Loss: 0.00966643905849196\n",
      "Epoch 92/100, Loss: 0.009705332893645391\n",
      "Epoch 93/100, Loss: 0.009741968679009005\n",
      "Epoch 94/100, Loss: 0.009692553197965026\n",
      "Epoch 95/100, Loss: 0.00970052435877733\n",
      "Epoch 96/100, Loss: 0.009715362364659086\n",
      "Epoch 97/100, Loss: 0.009625360544305295\n",
      "Epoch 98/100, Loss: 0.009674970526248217\n",
      "Epoch 99/100, Loss: 0.009721448121126741\n",
      "Epoch 100/100, Loss: 0.009777422645129263\n",
      "Eval Loss: 0.00975459921755828\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a simple linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Generate synthetic data for a linear function y = 2x + 3\n",
    "def generate_synthetic_data(samples):\n",
    "    x = torch.randn(samples, 1)\n",
    "    y = 2 * x + 3 + 0.1 * torch.randn(samples, 1)\n",
    "    return x, y\n",
    "\n",
    "# Define a custom dataset for the synthetic data\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# Train the model\n",
    "def train(model, dataloader, optimizer, loss_fn, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for x, y in dataloader:\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "# Set parameters\n",
    "samples = 1000\n",
    "batch_size = 64\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Generate and load synthetic data\n",
    "x, y = generate_synthetic_data(samples)\n",
    "dataset = SyntheticDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train(model, dataloader, optimizer, loss_fn, epochs)\n",
    "eval_loss = evaluate(model, dataloader, loss_fn)\n",
    "print(f\"Eval Loss: {eval_loss}\")\n",
    "\n",
    "# Plot the synthetic data and the learned function\n",
    "plt.scatter(x, y, label='Data')\n",
    "plt.plot(x, model(x).detach(), 'r', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d9b4b",
   "metadata": {},
   "source": [
    "# Train with the context manager!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57ea3fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "loss = torch.zeros(1)\n",
    "loss += torch.ones(1)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036605a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PyTorchPhaseManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m eval_loss \u001b[38;5;241m=\u001b[39m evaluate(model, dataloader, loss_fn)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m         epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#             optimizer.zero_grad()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPyTorchPhaseManager\u001b[49m(model, optimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m      9\u001b[0m                 outputs \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     10\u001b[0m                 loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PyTorchPhaseManager' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "def train(model, dataloader, optimizer, loss_fn, epochs):\n",
    "#     model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for x, y in dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "            loss = torch.zeros(1)\n",
    "            with PyTorchPhaseManager(model, optimizer, loss=loss, phase='training') as p:\n",
    "                outputs = model(x)\n",
    "                loss += loss_fn(outputs, y)\n",
    "                p.add_loss(loss)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    loss = torch.zeros(1)\n",
    "    for x, y in dataloader:\n",
    "        with PyTorchPhaseManager(model, optimizer, loss=0., phase='evaluation') as p:\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            p.add_loss(loss)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Set parameters\n",
    "samples = 1000\n",
    "batch_size = 64\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Generate and load synthetic data\n",
    "x, y = generate_synthetic_data(samples)\n",
    "dataset = SyntheticDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train(model, dataloader, optimizer, loss_fn, epochs)\n",
    "eval_loss = evaluate(model, dataloader, loss_fn)\n",
    "print(f\"Eval Loss: {eval_loss}\")\n",
    "\n",
    "# Plot the synthetic data and the learned function\n",
    "plt.scatter(x, y, label='Data')\n",
    "plt.plot(x, model(x).detach(), 'r', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d1fe4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PyTorchPhaseManager:\n",
    "    \n",
    "    def __init__(self, phase):\n",
    "        self.phase = phase\n",
    "        \n",
    "    def __enter__(self):\n",
    "        if 'train' in self.phase:\n",
    "            self.mode = torch.enable_grad()\n",
    "            self.training_preprocess()\n",
    "        elif 'val' in self.phase:\n",
    "            self.mode = torch.no_grad()\n",
    "            self.evaluation_preprocess()\n",
    "        else:\n",
    "            raise ValueError('Invalid phase: f{self.phase}')\n",
    "        \n",
    "        self.mode.__enter__()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        \n",
    "        if 'train' in self.phase:\n",
    "            self.training_postprocess()\n",
    "        elif 'val' in self.phase:\n",
    "            self.evaluation_postprocess()\n",
    "        else:\n",
    "            raise ValueError('Invalid phase: f{self.phase}')\n",
    "        \n",
    "        self.mode.__exit__(*args)\n",
    "        \n",
    "    def training_preprocess(self):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    def training_postprocess(self):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    def evaluation_preprocess(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluation_postprocess(self):\n",
    "        pass\n",
    "\n",
    "        \n",
    "# loss = 2.0        \n",
    "with PyTorchPhaseManager(model, optimizer, loss=loss, phase='evaluation') as p:\n",
    "    p.add_loss(loss)\n",
    "    p.tmp()\n",
    "p.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56163f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1058/3511460715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Train and evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Eval Loss: {eval_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1058/3511460715.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mPyTorchPhaseManager2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1058/3675177334.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'val'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1058/3675177334.py\u001b[0m in \u001b[0;36mtraining_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "def train(model, dataloader, optimizer, loss_fn, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for x, y in dataloader:\n",
    "            with PyTorchPhaseManager2(phase='training'):\n",
    "                outputs = model(x)\n",
    "                loss = loss_fn(outputs, y)\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    total_loss = 0.0\n",
    "    for x, y in dataloader:\n",
    "        with PyTorchPhaseManager2(phase='evaluation'):\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Set parameters\n",
    "samples = 1000\n",
    "batch_size = 64\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "# Generate and load synthetic data\n",
    "x, y = generate_synthetic_data(samples)\n",
    "dataset = SyntheticDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and evaluate the model\n",
    "train(model, dataloader, optimizer, loss_fn, epochs)\n",
    "eval_loss = evaluate(model, dataloader, loss_fn)\n",
    "print(f\"Eval Loss: {eval_loss}\")\n",
    "\n",
    "# Plot the synthetic data and the learned function\n",
    "plt.scatter(x, y, label='Data')\n",
    "plt.plot(x, model(x).detach(), 'r', label='Learned function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24218572",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PyTorchPhaseManager2(phase='training'):\n",
    "    outputs = model(x)\n",
    "    loss = loss_fn(outputs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b82aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class PyTorchPhaseManager:\n",
    "    '''\n",
    "    A context manager for managing the training and evaluation phases of a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "    - phase (str): The phase to manage ('train' or 'val').\n",
    "\n",
    "    Methods:\n",
    "    - __init__(self, phase)\n",
    "    - __enter__(self)\n",
    "    - __exit__(self, *args)\n",
    "    - training_preprocess(self)\n",
    "    - training_postprocess(self)\n",
    "    - evaluation_preprocess(self)\n",
    "    - evaluation_postprocess(self)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, phase):\n",
    "        '''\n",
    "        Initializes a PyTorchPhaseManager object with the specified phase.\n",
    "\n",
    "        Args:\n",
    "        - phase (str): The phase to manage ('train' or 'val').\n",
    "        '''\n",
    "        self.phase = phase\n",
    "\n",
    "    def __enter__(self):\n",
    "        '''\n",
    "        Enters the context for the current phase and performs any necessary preprocessing.\n",
    "\n",
    "        Returns:\n",
    "        - self: The PyTorchPhaseManager object.\n",
    "        '''\n",
    "        if 'train' in self.phase:\n",
    "            self.mode = torch.enable_grad()\n",
    "            self.training_preprocess()\n",
    "        elif 'val' in self.phase:\n",
    "            self.mode = torch.no_grad()\n",
    "            self.evaluation_preprocess()\n",
    "        else:\n",
    "            raise ValueError(f'Invalid phase: {self.phase}')\n",
    "\n",
    "        self.mode.__enter__()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        '''\n",
    "        Exits the context for the current phase and performs any necessary postprocessing.\n",
    "\n",
    "        Args:\n",
    "        - *args: The arguments passed to __exit__.\n",
    "        '''\n",
    "        if 'train' in self.phase:\n",
    "            self.training_postprocess()\n",
    "        elif 'val' in self.phase:\n",
    "            self.evaluation_postprocess()\n",
    "        else:\n",
    "            raise ValueError(f'Invalid phase: {self.phase}')\n",
    "\n",
    "        self.mode.__exit__(*args)\n",
    "\n",
    "    def training_preprocess(self):\n",
    "        '''\n",
    "        Sets the model to training mode and clears the gradients of the optimizer.\n",
    "        '''\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        print('train prep DONE!')\n",
    "\n",
    "    def training_postprocess(self):\n",
    "        '''\n",
    "        Computes the gradients of the loss with respect to the model parameters using loss.backward()\n",
    "        and performs the optimization step using optimizer.step().\n",
    "        '''\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('exiting train!')\n",
    "\n",
    "    def evaluation_preprocess(self):\n",
    "        '''\n",
    "        Sets the model to evaluation mode.\n",
    "        '''\n",
    "        model.eval()\n",
    "        print('eval prep DONE!')\n",
    "\n",
    "    def evaluation_postprocess(self):\n",
    "        '''\n",
    "        Performs no post-processing steps, as gradients are not computed during evaluation.\n",
    "        '''\n",
    "        print('exiting eval!')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47aed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the training parameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                               transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                              transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d05f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56978a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train prep DONE!\n",
      "exiting train!\n"
     ]
    }
   ],
   "source": [
    "with PyTorchPhaseManager(phase='train'):\n",
    "    # Perform a forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "463e1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n",
      "train prep DONE!\n",
      "True\n",
      "exiting train!\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "for i, (inputs, targets) in enumerate(train_loader):\n",
    "    if i == 50:\n",
    "        break\n",
    "    with PyTorchPhaseManager(phase='train'):\n",
    "        print(torch.is_grad_enabled())\n",
    "        # Perform a forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        \n",
    "    # Update the running loss and accuracy\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_corrects += torch.sum(preds == targets.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8662a830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n",
      "eval prep DONE!\n",
      "False\n",
      "exiting eval!\n"
     ]
    }
   ],
   "source": [
    "running_corrects = 0\n",
    "for i, (inputs, targets) in enumerate(test_loader):\n",
    "    if i == 50:\n",
    "        break\n",
    "    with PyTorchPhaseManager(phase='evaluation'):\n",
    "        print(torch.is_grad_enabled())\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == targets.data)\n",
    "test_acc = running_corrects.double() / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    for inputs, targets in test_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == targets.data)\n",
    "\n",
    "    test_acc = running_corrects.double() / len(test_loader.dataset)\n",
    "\n",
    "    print('Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76daecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1147/2628479693.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# assert x_batch is not None, 'x_batch not found. Note that the name must be \"x_batch.\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_batch' is not defined"
     ]
    }
   ],
   "source": [
    "assert model is not None\n",
    "# assert x_batch is not None, 'x_batch not found. Note that the name must be \"x_batch.\"'\n",
    "locals()\n",
    "assert x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771240e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2510d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5debb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c18c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8133effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.model=tensor([1, 2, 3])\n",
      "self.model=tensor([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from MyManager import MyManager\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# model = np.array([1, 2, 3])\n",
    "model = torch.tensor([1, 2, 3])\n",
    "with MyManager(model=model):\n",
    "    model += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65df4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
