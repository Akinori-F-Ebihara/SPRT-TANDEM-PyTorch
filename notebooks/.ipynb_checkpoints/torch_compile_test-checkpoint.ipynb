{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f5d039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 11:57:50.891815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX_VNNI\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-27 11:57:50.962341: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os, pdb\n",
    "sys.path.append('/home/afe/Dropbox/PYTHON/SPRTproject/TANDEMAUS/')\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "from loguru import logger\n",
    "from config.config import config\n",
    "from utils.misc import grab_gpu, fix_random_seed, parse_args\n",
    "from utils.logging import create_log_folders, save_config_info, ContexualLogger,\\\n",
    "                          save_sdre_imgs, log_training_results, get_tb_writer\n",
    "from utils.hyperparameter_tuning import run_optuna, suggest_parameters_optuna, report_to_pruner\n",
    "from utils.checkpoint import update_and_save_result, \\\n",
    "                             initialize_objectives, finalize_objectives\n",
    "from datasets.data_processing import load_lmdb_dataloaders\n",
    "from models.temporal_integrators import import_model\n",
    "from models.optimizers import initialize_optimizer\n",
    "from models.losses import compute_loss_and_metrics\n",
    "from utils.misc import PyTorchPhaseManager, convert_torch_to_numpy\n",
    "from utils.performance_metrics import training_setup, accumulate_performance\n",
    "import torch\n",
    "config['MODEL_BACKBONE'] = 'Transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d50116",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 11:57:52.142 | INFO     | models.temporal_integrators:summarize:354 - \n",
      ":'######::'########::'########::'########::::::::::::::::::::::\n",
      "'##... ##: ##.... ##: ##.... ##:... ##..:::::::::::::::::::::::\n",
      " ##:::..:: ##:::: ##: ##:::: ##:::: ##:::::::::::::::::::::::::\n",
      ". ######:: ########:: ########::::: ##::::'#######:::::::::::::\n",
      ":..... ##: ##.....::: ##.. ##:::::: ##::::........:::::::::::::\n",
      "'##::: ##: ##:::::::: ##::. ##::::: ##:::::::::::::::::::::::::\n",
      ". ######:: ##:::::::: ##:::. ##:::: ##:::::::::::::::::::::::::\n",
      ":......:::..:::::::::..:::::..:::::..::::::::::::::::::::::::::\n",
      "'########::::'###::::'##::: ##:'########::'########:'##::::'##:\n",
      "... ##..::::'## ##::: ###:: ##: ##.... ##: ##.....:: ###::'###:\n",
      "::: ##:::::'##:. ##:: ####: ##: ##:::: ##: ##::::::: ####'####:\n",
      "::: ##::::'##:::. ##: ## ## ##: ##:::: ##: ######::: ## ### ##:\n",
      "::: ##:::: #########: ##. ####: ##:::: ##: ##...:::: ##. #: ##:\n",
      "::: ##:::: ##.... ##: ##:. ###: ##:::: ##: ##::::::: ##:.:: ##:\n",
      "::: ##:::: ##:::: ##: ##::. ##: ########:: ########: ##:::: ##:\n",
      ":::..:::::..:::::..::..::::..::........:::........::..:::::..::\n",
      "\n",
      "2023-03-27 11:57:52.142 | INFO     | models.temporal_integrators:summarize:356 - \u001b[34mofficial PyTorch implementation.\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/usr/local/lib/python3.10/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n",
      "2023-03-27 11:57:52.878 | INFO     | models.temporal_integrators:summarize:368 - Network summary:\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "TANDEMformer                                  [100, 2, 3]               128\n",
      "├─PositionalEncoding: 1-1                     [100, 1, 128]             --\n",
      "│    └─Dropout: 2-1                           [1, 100, 128]             --\n",
      "├─TransformerEncoder: 1-2                     [100, 1, 128]             --\n",
      "│    └─ModuleList: 2-4                        --                        (recursive)\n",
      "│    │    └─TransformerEncoderLayer: 3-1      [100, 1, 128]             83,136\n",
      "│    │    └─TransformerEncoderLayer: 3-2      [100, 1, 128]             83,136\n",
      "├─Linear: 1-3                                 [100, 64]                 8,256\n",
      "├─Dropout: 1-4                                [100, 64]                 --\n",
      "├─PositionalEncoding: 1-5                     [100, 2, 128]             --\n",
      "│    └─Dropout: 2-3                           [2, 100, 128]             --\n",
      "├─TransformerEncoder: 1-6                     [100, 2, 128]             (recursive)\n",
      "│    └─ModuleList: 2-4                        --                        (recursive)\n",
      "│    │    └─TransformerEncoderLayer: 3-3      [100, 2, 128]             (recursive)\n",
      "│    │    └─TransformerEncoderLayer: 3-4      [100, 2, 128]             (recursive)\n",
      "├─Linear: 1-7                                 [100, 64]                 (recursive)\n",
      "├─Dropout: 1-8                                [100, 64]                 --\n",
      "├─ReLU: 1-9                                   [100, 64]                 --\n",
      "├─Linear: 1-10                                [100, 3]                  195\n",
      "├─ReLU: 1-11                                  [100, 64]                 --\n",
      "├─Linear: 1-12                                [100, 3]                  (recursive)\n",
      "===============================================================================================\n",
      "Total params: 174,851\n",
      "Trainable params: 174,851\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.69\n",
      "===============================================================================================\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.24\n",
      "===============================================================================================\n",
      "2023-03-27 11:57:52.879 | INFO     | models.temporal_integrators:summarize:369 - Example input shape: torch.Size([100, 2, 128])\n",
      "2023-03-27 11:57:52.879 | INFO     | models.temporal_integrators:summarize:370 - Example output shape: torch.Size([100, 2, 3])\n",
      "2023-03-27 11:57:52.880 | INFO     | models.temporal_integrators:import_model:56 - model moved onto: \u001b[33mcuda.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "# initialize the network or load a pretrained one\n",
    "model = import_model(config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ed0250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TANDEMformer(\n",
       "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (activation_logit): ReLU()\n",
       "    (decoder): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (fc_logits): Linear(in_features=64, out_features=3, bias=True)\n",
       "    (pos_encoder): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_model = torch.compile(model)\n",
    "opt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4085afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the optimizer\n",
    "optimizer, schedular = initialize_optimizer(model, config)\n",
    "\n",
    "# load train, val, and test data\n",
    "data_loaders = load_lmdb_dataloaders(config, device)\n",
    "\n",
    "# try loading after saving .pt file!\n",
    "# checkpoint = torch.load('./tmp.pt')\n",
    "# opt_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0acd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 ?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0683, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "global_step = 0\n",
    "\n",
    "is_train, iter_num, performance_metrics, barcolor = training_setup(phase, config)\n",
    "\n",
    "for local_step, data in tqdm(enumerate(data_loaders[phase]),\n",
    "                                           mininterval=2, total=iter_num,\n",
    "                                           desc=colored(f'{phase} in progress...', barcolor),\n",
    "                                           colour=barcolor, leave=False):\n",
    "    if len(data) == 3:\n",
    "        # if the dataset contains ground-truth log likelihood ratio\n",
    "        x_batch, y_batch, gt_llrs_batch = data\n",
    "    elif len(data) == 2:\n",
    "        # data and label only: typical real-world data\n",
    "        x_batch, y_batch = data\n",
    "    else:\n",
    "        raise ValueError('data tuple length is expected either to be '\n",
    "                        f'3 (x, y, llr) or 2 (x, y) but got {len(data)=}!')\n",
    "        \n",
    "    with PyTorchPhaseManager(model, optimizer, phase=phase) as p:\n",
    "            p.loss, monitored_values = compute_loss_and_metrics(\n",
    "                p.model, x_batch, y_batch, global_step, config=config)\n",
    "    \n",
    "    print(p.loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "915d02b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': opt_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    \n",
    "}, './tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db8640c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./tmp.pt')\n",
    "opt_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18833ecf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TANDEMformer:\n\tMissing key(s) in state_dict: \"layer_norm.weight\", \"layer_norm.bias\", \"decoder.weight\", \"decoder.bias\", \"fc_logits.weight\", \"fc_logits.bias\", \"pos_encoder.pe\", \"transformer_encoder.layers.0.self_attn.in_proj_weight\", \"transformer_encoder.layers.0.self_attn.in_proj_bias\", \"transformer_encoder.layers.0.self_attn.out_proj.weight\", \"transformer_encoder.layers.0.self_attn.out_proj.bias\", \"transformer_encoder.layers.0.linear1.weight\", \"transformer_encoder.layers.0.linear1.bias\", \"transformer_encoder.layers.0.linear2.weight\", \"transformer_encoder.layers.0.linear2.bias\", \"transformer_encoder.layers.0.norm1.weight\", \"transformer_encoder.layers.0.norm1.bias\", \"transformer_encoder.layers.0.norm2.weight\", \"transformer_encoder.layers.0.norm2.bias\", \"transformer_encoder.layers.1.self_attn.in_proj_weight\", \"transformer_encoder.layers.1.self_attn.in_proj_bias\", \"transformer_encoder.layers.1.self_attn.out_proj.weight\", \"transformer_encoder.layers.1.self_attn.out_proj.bias\", \"transformer_encoder.layers.1.linear1.weight\", \"transformer_encoder.layers.1.linear1.bias\", \"transformer_encoder.layers.1.linear2.weight\", \"transformer_encoder.layers.1.linear2.bias\", \"transformer_encoder.layers.1.norm1.weight\", \"transformer_encoder.layers.1.norm1.bias\", \"transformer_encoder.layers.1.norm2.weight\", \"transformer_encoder.layers.1.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"_orig_mod.layer_norm.weight\", \"_orig_mod.layer_norm.bias\", \"_orig_mod.decoder.weight\", \"_orig_mod.decoder.bias\", \"_orig_mod.fc_logits.weight\", \"_orig_mod.fc_logits.bias\", \"_orig_mod.pos_encoder.pe\", \"_orig_mod.transformer_encoder.layers.0.self_attn.in_proj_weight\", \"_orig_mod.transformer_encoder.layers.0.self_attn.in_proj_bias\", \"_orig_mod.transformer_encoder.layers.0.self_attn.out_proj.weight\", \"_orig_mod.transformer_encoder.layers.0.self_attn.out_proj.bias\", \"_orig_mod.transformer_encoder.layers.0.linear1.weight\", \"_orig_mod.transformer_encoder.layers.0.linear1.bias\", \"_orig_mod.transformer_encoder.layers.0.linear2.weight\", \"_orig_mod.transformer_encoder.layers.0.linear2.bias\", \"_orig_mod.transformer_encoder.layers.0.norm1.weight\", \"_orig_mod.transformer_encoder.layers.0.norm1.bias\", \"_orig_mod.transformer_encoder.layers.0.norm2.weight\", \"_orig_mod.transformer_encoder.layers.0.norm2.bias\", \"_orig_mod.transformer_encoder.layers.1.self_attn.in_proj_weight\", \"_orig_mod.transformer_encoder.layers.1.self_attn.in_proj_bias\", \"_orig_mod.transformer_encoder.layers.1.self_attn.out_proj.weight\", \"_orig_mod.transformer_encoder.layers.1.self_attn.out_proj.bias\", \"_orig_mod.transformer_encoder.layers.1.linear1.weight\", \"_orig_mod.transformer_encoder.layers.1.linear1.bias\", \"_orig_mod.transformer_encoder.layers.1.linear2.weight\", \"_orig_mod.transformer_encoder.layers.1.linear2.bias\", \"_orig_mod.transformer_encoder.layers.1.norm1.weight\", \"_orig_mod.transformer_encoder.layers.1.norm1.bias\", \"_orig_mod.transformer_encoder.layers.1.norm2.weight\", \"_orig_mod.transformer_encoder.layers.1.norm2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_103/842573518.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TANDEMformer:\n\tMissing key(s) in state_dict: \"layer_norm.weight\", \"layer_norm.bias\", \"decoder.weight\", \"decoder.bias\", \"fc_logits.weight\", \"fc_logits.bias\", \"pos_encoder.pe\", \"transformer_encoder.layers.0.self_attn.in_proj_weight\", \"transformer_encoder.layers.0.self_attn.in_proj_bias\", \"transformer_encoder.layers.0.self_attn.out_proj.weight\", \"transformer_encoder.layers.0.self_attn.out_proj.bias\", \"transformer_encoder.layers.0.linear1.weight\", \"transformer_encoder.layers.0.linear1.bias\", \"transformer_encoder.layers.0.linear2.weight\", \"transformer_encoder.layers.0.linear2.bias\", \"transformer_encoder.layers.0.norm1.weight\", \"transformer_encoder.layers.0.norm1.bias\", \"transformer_encoder.layers.0.norm2.weight\", \"transformer_encoder.layers.0.norm2.bias\", \"transformer_encoder.layers.1.self_attn.in_proj_weight\", \"transformer_encoder.layers.1.self_attn.in_proj_bias\", \"transformer_encoder.layers.1.self_attn.out_proj.weight\", \"transformer_encoder.layers.1.self_attn.out_proj.bias\", \"transformer_encoder.layers.1.linear1.weight\", \"transformer_encoder.layers.1.linear1.bias\", \"transformer_encoder.layers.1.linear2.weight\", \"transformer_encoder.layers.1.linear2.bias\", \"transformer_encoder.layers.1.norm1.weight\", \"transformer_encoder.layers.1.norm1.bias\", \"transformer_encoder.layers.1.norm2.weight\", \"transformer_encoder.layers.1.norm2.bias\". \n\tUnexpected key(s) in state_dict: \"_orig_mod.layer_norm.weight\", \"_orig_mod.layer_norm.bias\", \"_orig_mod.decoder.weight\", \"_orig_mod.decoder.bias\", \"_orig_mod.fc_logits.weight\", \"_orig_mod.fc_logits.bias\", \"_orig_mod.pos_encoder.pe\", \"_orig_mod.transformer_encoder.layers.0.self_attn.in_proj_weight\", \"_orig_mod.transformer_encoder.layers.0.self_attn.in_proj_bias\", \"_orig_mod.transformer_encoder.layers.0.self_attn.out_proj.weight\", \"_orig_mod.transformer_encoder.layers.0.self_attn.out_proj.bias\", \"_orig_mod.transformer_encoder.layers.0.linear1.weight\", \"_orig_mod.transformer_encoder.layers.0.linear1.bias\", \"_orig_mod.transformer_encoder.layers.0.linear2.weight\", \"_orig_mod.transformer_encoder.layers.0.linear2.bias\", \"_orig_mod.transformer_encoder.layers.0.norm1.weight\", \"_orig_mod.transformer_encoder.layers.0.norm1.bias\", \"_orig_mod.transformer_encoder.layers.0.norm2.weight\", \"_orig_mod.transformer_encoder.layers.0.norm2.bias\", \"_orig_mod.transformer_encoder.layers.1.self_attn.in_proj_weight\", \"_orig_mod.transformer_encoder.layers.1.self_attn.in_proj_bias\", \"_orig_mod.transformer_encoder.layers.1.self_attn.out_proj.weight\", \"_orig_mod.transformer_encoder.layers.1.self_attn.out_proj.bias\", \"_orig_mod.transformer_encoder.layers.1.linear1.weight\", \"_orig_mod.transformer_encoder.layers.1.linear1.bias\", \"_orig_mod.transformer_encoder.layers.1.linear2.weight\", \"_orig_mod.transformer_encoder.layers.1.linear2.bias\", \"_orig_mod.transformer_encoder.layers.1.norm1.weight\", \"_orig_mod.transformer_encoder.layers.1.norm1.bias\", \"_orig_mod.transformer_encoder.layers.1.norm2.weight\", \"_orig_mod.transformer_encoder.layers.1.norm2.bias\". "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
