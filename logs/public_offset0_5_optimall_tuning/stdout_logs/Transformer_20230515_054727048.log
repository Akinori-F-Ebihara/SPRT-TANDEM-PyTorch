2023-05-15 05:47:27.060 | INFO     | utils.misc:fix_random_seed:405 - Random seed is not fixed.
2023-05-15 05:47:27.072 | INFO     | models.temporal_integrators:summarize:566 - 
:'######::'########::'########::'########::::::::::::::::::::::
'##... ##: ##.... ##: ##.... ##:... ##..:::::::::::::::::::::::
 ##:::..:: ##:::: ##: ##:::: ##:::: ##:::::::::::::::::::::::::
. ######:: ########:: ########::::: ##::::'#######:::::::::::::
:..... ##: ##.....::: ##.. ##:::::: ##::::........:::::::::::::
'##::: ##: ##:::::::: ##::. ##::::: ##:::::::::::::::::::::::::
. ######:: ##:::::::: ##:::. ##:::: ##:::::::::::::::::::::::::
:......:::..:::::::::..:::::..:::::..::::::::::::::::::::::::::
'########::::'###::::'##::: ##:'########::'########:'##::::'##:
... ##..::::'## ##::: ###:: ##: ##.... ##: ##.....:: ###::'###:
::: ##:::::'##:. ##:: ####: ##: ##:::: ##: ##::::::: ####'####:
::: ##::::'##:::. ##: ## ## ##: ##:::: ##: ######::: ## ### ##:
::: ##:::: #########: ##. ####: ##:::: ##: ##...:::: ##. #: ##:
::: ##:::: ##.... ##: ##:. ###: ##:::: ##: ##::::::: ##:.:: ##:
::: ##:::: ##:::: ##: ##::. ##: ########:: ########: ##:::: ##:
:::..:::::..:::::..::..::::..::........:::........::..:::::..::

2023-05-15 05:47:27.072 | INFO     | models.temporal_integrators:summarize:569 - [34mofficial PyTorch implementation.[0m
2023-05-15 05:47:27.160 | INFO     | models.temporal_integrators:summarize:591 - Network summary:
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
TANDEMformer                                  [150, 50, 3, 3]           640
â”œâ”€TransformerEncoder: 1-1                     [6900, 1, 128]            --
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-1      [6900, 1, 128]            74,912
â”œâ”€Linear: 1-2                                 [6900, 64]                8,256
â”œâ”€Dropout: 1-3                                [6900, 64]                --
â”œâ”€TransformerEncoder: 1-4                     [6900, 2, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-2      [6900, 2, 128]            (recursive)
â”œâ”€Linear: 1-5                                 [6900, 64]                (recursive)
â”œâ”€Dropout: 1-6                                [6900, 64]                --
â”œâ”€TransformerEncoder: 1-7                     [6900, 3, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-3      [6900, 3, 128]            (recursive)
â”œâ”€Linear: 1-8                                 [6900, 64]                (recursive)
â”œâ”€Dropout: 1-9                                [6900, 64]                --
â”œâ”€TransformerEncoder: 1-10                    [6900, 4, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-4      [6900, 4, 128]            (recursive)
â”œâ”€Linear: 1-11                                [6900, 64]                (recursive)
â”œâ”€Dropout: 1-12                               [6900, 64]                --
â”œâ”€TransformerEncoder: 1-13                    [6900, 5, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-5      [6900, 5, 128]            (recursive)
â”œâ”€Linear: 1-14                                [6900, 64]                (recursive)
â”œâ”€Dropout: 1-15                               [6900, 64]                --
â”œâ”€Tanh: 1-16                                  [6900, 64]                --
â”œâ”€Linear: 1-17                                [6900, 3]                 195
â”œâ”€Tanh: 1-18                                  [6900, 64]                --
â”œâ”€Linear: 1-19                                [6900, 3]                 (recursive)
â”œâ”€Tanh: 1-20                                  [6900, 64]                --
â”œâ”€Linear: 1-21                                [6900, 3]                 (recursive)
â”œâ”€Tanh: 1-22                                  [6900, 64]                --
â”œâ”€Linear: 1-23                                [6900, 3]                 (recursive)
â”œâ”€Tanh: 1-24                                  [6900, 64]                --
â”œâ”€Linear: 1-25                                [6900, 3]                 (recursive)
===============================================================================================
Total params: 84,003
Trainable params: 84,003
Non-trainable params: 0
Total mult-adds (M): 597.37
===============================================================================================
Input size (MB): 3.84
Forward/backward pass size (MB): 362.94
Params size (MB): 0.07
Estimated Total Size (MB): 366.85
===============================================================================================
2023-05-15 05:47:27.161 | INFO     | models.temporal_integrators:summarize:592 - Example input shape: torch.Size([150, 50, 128])
2023-05-15 05:47:27.161 | INFO     | models.temporal_integrators:summarize:593 - Example output shape: torch.Size([150, 50, 3, 3])
2023-05-15 05:47:27.163 | INFO     | models.temporal_integrators:import_model:58 - model moved onto: [33mcuda.[0m
2023-05-15 05:47:27.163 | INFO     | models.optimizers:initialize_optimizer:44 - Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0002053820668429108
    maximize: False
    weight_decay: 0.0008
)
2023-05-15 05:47:27.164 | INFO     | datasets.data_processing:lmdb_dataloaders:293 - loading data... 
2023-05-15 05:47:27.164 | INFO     | datasets.data_processing:lmdb_dataloaders:295 - If this process takes long, consider setting is_load_onto_memory=False or use LMDBIterableDataset.
2023-05-15 05:47:29.405 | INFO     | __main__:objective:58 - Starting epoch #0.
2023-05-15 05:47:31.775 | INFO     | utils.logging:log_training_results:280 - 
2023-05-15 05:47:31.776 | INFO     | utils.logging:log_training_results:281 - Global Step =      0
2023-05-15 05:47:31.776 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.29238665103912354
2023-05-15 05:47:31.777 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 3.527456760406494
2023-05-15 05:47:31.777 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.2719571590423584
2023-05-15 05:47:31.777 | INFO     | utils.logging:log_training_results:326 - val MCE loss:1.10629 * 0.1
2023-05-15 05:47:31.777 | INFO     | utils.logging:log_training_results:333 - val LLLR :0.37875 * 0.0
2023-05-15 05:47:31.778 | INFO     | utils.logging:log_training_results:340 - val weight decay:1170.08203 * 0.0008
2023-05-15 05:47:32.561 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-15 05:47:32.894 | INFO     | utils.logging:log_training_results:280 - 
2023-05-15 05:47:32.895 | INFO     | utils.logging:log_training_results:281 - Global Step =      0
2023-05-15 05:47:32.895 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.2931859791278839
2023-05-15 05:47:32.895 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 3.7634782791137695
2023-05-15 05:47:32.896 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.28427159786224365
2023-05-15 05:47:32.896 | INFO     | utils.logging:log_training_results:326 - train MCE loss:1.10951 * 0.1
2023-05-15 05:47:32.896 | INFO     | utils.logging:log_training_results:333 - train LLLR :0.38211 * 0.0
2023-05-15 05:47:32.896 | INFO     | utils.logging:log_training_results:340 - train weight decay:1170.08203 * 0.0008
