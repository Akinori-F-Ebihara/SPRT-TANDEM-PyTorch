2023-05-16 04:27:07.571 | INFO     | utils.misc:fix_random_seed:406 - Random seed is not fixed.
2023-05-16 04:27:07.588 | INFO     | models.temporal_integrators:summarize:566 - 
:'######::'########::'########::'########::::::::::::::::::::::
'##... ##: ##.... ##: ##.... ##:... ##..:::::::::::::::::::::::
 ##:::..:: ##:::: ##: ##:::: ##:::: ##:::::::::::::::::::::::::
. ######:: ########:: ########::::: ##::::'#######:::::::::::::
:..... ##: ##.....::: ##.. ##:::::: ##::::........:::::::::::::
'##::: ##: ##:::::::: ##::. ##::::: ##:::::::::::::::::::::::::
. ######:: ##:::::::: ##:::. ##:::: ##:::::::::::::::::::::::::
:......:::..:::::::::..:::::..:::::..::::::::::::::::::::::::::
'########::::'###::::'##::: ##:'########::'########:'##::::'##:
... ##..::::'## ##::: ###:: ##: ##.... ##: ##.....:: ###::'###:
::: ##:::::'##:. ##:: ####: ##: ##:::: ##: ##::::::: ####'####:
::: ##::::'##:::. ##: ## ## ##: ##:::: ##: ######::: ## ### ##:
::: ##:::: #########: ##. ####: ##:::: ##: ##...:::: ##. #: ##:
::: ##:::: ##.... ##: ##:. ###: ##:::: ##: ##::::::: ##:.:: ##:
::: ##:::: ##:::: ##: ##::. ##: ########:: ########: ##:::: ##:
:::..:::::..:::::..::..::::..::........:::........::..:::::..::

2023-05-16 04:27:07.589 | INFO     | models.temporal_integrators:summarize:569 - [34mofficial PyTorch implementation.[0m
2023-05-16 04:27:09.143 | INFO     | models.temporal_integrators:summarize:591 - Network summary:
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
TANDEMformer                                  [150, 50, 3, 3]           640
â”œâ”€TransformerEncoder: 1-1                     [6900, 1, 128]            --
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-1      [6900, 1, 128]            74,912
â”œâ”€Linear: 1-2                                 [6900, 64]                8,256
â”œâ”€Dropout: 1-3                                [6900, 64]                --
â”œâ”€TransformerEncoder: 1-4                     [6900, 2, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-2      [6900, 2, 128]            (recursive)
â”œâ”€Linear: 1-5                                 [6900, 64]                (recursive)
â”œâ”€Dropout: 1-6                                [6900, 64]                --
â”œâ”€TransformerEncoder: 1-7                     [6900, 3, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-3      [6900, 3, 128]            (recursive)
â”œâ”€Linear: 1-8                                 [6900, 64]                (recursive)
â”œâ”€Dropout: 1-9                                [6900, 64]                --
â”œâ”€TransformerEncoder: 1-10                    [6900, 4, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-4      [6900, 4, 128]            (recursive)
â”œâ”€Linear: 1-11                                [6900, 64]                (recursive)
â”œâ”€Dropout: 1-12                               [6900, 64]                --
â”œâ”€TransformerEncoder: 1-13                    [6900, 5, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-5                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-5      [6900, 5, 128]            (recursive)
â”œâ”€Linear: 1-14                                [6900, 64]                (recursive)
â”œâ”€Dropout: 1-15                               [6900, 64]                --
â”œâ”€Tanh: 1-16                                  [6900, 64]                --
â”œâ”€Linear: 1-17                                [6900, 3]                 195
â”œâ”€Tanh: 1-18                                  [6900, 64]                --
â”œâ”€Linear: 1-19                                [6900, 3]                 (recursive)
â”œâ”€Tanh: 1-20                                  [6900, 64]                --
â”œâ”€Linear: 1-21                                [6900, 3]                 (recursive)
â”œâ”€Tanh: 1-22                                  [6900, 64]                --
â”œâ”€Linear: 1-23                                [6900, 3]                 (recursive)
â”œâ”€Tanh: 1-24                                  [6900, 64]                --
â”œâ”€Linear: 1-25                                [6900, 3]                 (recursive)
===============================================================================================
Total params: 84,003
Trainable params: 84,003
Non-trainable params: 0
Total mult-adds (M): 597.37
===============================================================================================
Input size (MB): 3.84
Forward/backward pass size (MB): 362.94
Params size (MB): 0.07
Estimated Total Size (MB): 366.85
===============================================================================================
2023-05-16 04:27:09.144 | INFO     | models.temporal_integrators:summarize:592 - Example input shape: torch.Size([150, 50, 128])
2023-05-16 04:27:09.144 | INFO     | models.temporal_integrators:summarize:593 - Example output shape: torch.Size([150, 50, 3, 3])
2023-05-16 04:27:09.146 | INFO     | models.temporal_integrators:import_model:58 - model moved onto: [33mcuda.[0m
2023-05-16 04:27:09.147 | INFO     | models.optimizers:initialize_optimizer:44 - Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 1.5659469341058192e-05
    maximize: False
    weight_decay: 0.0004
)
2023-05-16 04:27:09.147 | INFO     | datasets.data_processing:lmdb_dataloaders:293 - loading data... 
2023-05-16 04:27:09.148 | INFO     | datasets.data_processing:lmdb_dataloaders:295 - If this process takes long, consider setting is_load_onto_memory=False or use LMDBIterableDataset.
2023-05-16 04:27:11.596 | INFO     | __main__:objective:58 - Starting epoch #0.
2023-05-16 04:27:13.877 | INFO     | utils.logging:log_training_results:280 - 
2023-05-16 04:27:13.877 | INFO     | utils.logging:log_training_results:281 - Global Step =      0
2023-05-16 04:27:13.878 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.32152998447418213
2023-05-16 04:27:13.878 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 3.5122296810150146
2023-05-16 04:27:13.878 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.31919389963150024
2023-05-16 04:27:13.879 | INFO     | utils.logging:log_training_results:326 - val MCE loss:1.10502 * 0.1
2023-05-16 04:27:13.879 | INFO     | utils.logging:log_training_results:333 - val LLLR :0.37656 * 0.0
2023-05-16 04:27:13.879 | INFO     | utils.logging:log_training_results:340 - val weight decay:1217.54675 * 0.0004
2023-05-16 04:27:15.007 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-16 04:27:15.323 | INFO     | utils.logging:log_training_results:280 - 
2023-05-16 04:27:15.323 | INFO     | utils.logging:log_training_results:281 - Global Step =      0
2023-05-16 04:27:15.323 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.28598546981811523
2023-05-16 04:27:15.324 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 3.7720651626586914
2023-05-16 04:27:15.324 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.2327888309955597
2023-05-16 04:27:15.324 | INFO     | utils.logging:log_training_results:326 - train MCE loss:1.10661 * 0.1
2023-05-16 04:27:15.325 | INFO     | utils.logging:log_training_results:333 - train LLLR :0.38304 * 0.0
2023-05-16 04:27:15.325 | INFO     | utils.logging:log_training_results:340 - train weight decay:1217.54675 * 0.0004
