2023-05-01 12:57:16.547 | INFO     | utils.misc:fix_random_seed:405 - Random seed is not fixed.
2023-05-01 12:57:16.567 | INFO     | models.temporal_integrators:summarize:568 - 
:'######::'########::'########::'########::::::::::::::::::::::
'##... ##: ##.... ##: ##.... ##:... ##..:::::::::::::::::::::::
 ##:::..:: ##:::: ##: ##:::: ##:::: ##:::::::::::::::::::::::::
. ######:: ########:: ########::::: ##::::'#######:::::::::::::
:..... ##: ##.....::: ##.. ##:::::: ##::::........:::::::::::::
'##::: ##: ##:::::::: ##::. ##::::: ##:::::::::::::::::::::::::
. ######:: ##:::::::: ##:::. ##:::: ##:::::::::::::::::::::::::
:......:::..:::::::::..:::::..:::::..::::::::::::::::::::::::::
'########::::'###::::'##::: ##:'########::'########:'##::::'##:
... ##..::::'## ##::: ###:: ##: ##.... ##: ##.....:: ###::'###:
::: ##:::::'##:. ##:: ####: ##: ##:::: ##: ##::::::: ####'####:
::: ##::::'##:::. ##: ## ## ##: ##:::: ##: ######::: ## ### ##:
::: ##:::: #########: ##. ####: ##:::: ##: ##...:::: ##. #: ##:
::: ##:::: ##.... ##: ##:. ###: ##:::: ##: ##::::::: ##:.:: ##:
::: ##:::: ##:::: ##: ##::. ##: ########:: ########: ##:::: ##:
:::..:::::..:::::..::..::::..::........:::........::..:::::..::

2023-05-01 12:57:16.568 | INFO     | models.temporal_integrators:summarize:571 - [34mofficial PyTorch implementation.[0m
2023-05-01 12:57:17.031 | INFO     | models.temporal_integrators:summarize:593 - Network summary:
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
TANDEMformer                                  [150, 50, 3, 3]           771
â”œâ”€TransformerEncoder: 1-1                     [6750, 1, 128]            --
â”‚    â””â”€ModuleList: 2-6                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-1      [6750, 1, 128]            83,136
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-2      [6750, 1, 128]            83,136
â”œâ”€Linear: 1-2                                 [6750, 64]                8,256
â”œâ”€Dropout: 1-3                                [6750, 64]                --
â”œâ”€TransformerEncoder: 1-4                     [6750, 2, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-6                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-3      [6750, 2, 128]            (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-4      [6750, 2, 128]            (recursive)
â”œâ”€Linear: 1-5                                 [6750, 64]                (recursive)
â”œâ”€Dropout: 1-6                                [6750, 64]                --
â”œâ”€TransformerEncoder: 1-7                     [6750, 3, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-6                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-5      [6750, 3, 128]            (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-6      [6750, 3, 128]            (recursive)
â”œâ”€Linear: 1-8                                 [6750, 64]                (recursive)
â”œâ”€Dropout: 1-9                                [6750, 64]                --
â”œâ”€TransformerEncoder: 1-10                    [6750, 4, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-6                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-7      [6750, 4, 128]            (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-8      [6750, 4, 128]            (recursive)
â”œâ”€Linear: 1-11                                [6750, 64]                (recursive)
â”œâ”€Dropout: 1-12                               [6750, 64]                --
â”œâ”€TransformerEncoder: 1-13                    [6750, 5, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-6                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-9      [6750, 5, 128]            (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-10     [6750, 5, 128]            (recursive)
â”œâ”€Linear: 1-14                                [6750, 64]                (recursive)
â”œâ”€Dropout: 1-15                               [6750, 64]                --
â”œâ”€TransformerEncoder: 1-16                    [6750, 6, 128]            (recursive)
â”‚    â””â”€ModuleList: 2-6                        --                        (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-11     [6750, 6, 128]            (recursive)
â”‚    â”‚    â””â”€TransformerEncoderLayer: 3-12     [6750, 6, 128]            (recursive)
â”œâ”€Linear: 1-17                                [6750, 64]                (recursive)
â”œâ”€Dropout: 1-18                               [6750, 64]                --
â”œâ”€Linear: 1-19                                [6750, 3]                 195
â”œâ”€Linear: 1-20                                [6750, 3]                 (recursive)
â”œâ”€Linear: 1-21                                [6750, 3]                 (recursive)
â”œâ”€Linear: 1-22                                [6750, 3]                 (recursive)
â”œâ”€Linear: 1-23                                [6750, 3]                 (recursive)
â”œâ”€Linear: 1-24                                [6750, 3]                 (recursive)
===============================================================================================
Total params: 175,494
Trainable params: 175,494
Non-trainable params: 0
Total mult-adds (G): 1.73
===============================================================================================
Input size (MB): 3.84
Forward/backward pass size (MB): 1037.77
Params size (MB): 0.17
Estimated Total Size (MB): 1041.78
===============================================================================================
2023-05-01 12:57:17.031 | INFO     | models.temporal_integrators:summarize:594 - Example input shape: torch.Size([150, 50, 128])
2023-05-01 12:57:17.032 | INFO     | models.temporal_integrators:summarize:595 - Example output shape: torch.Size([150, 50, 3, 3])
2023-05-01 12:57:17.034 | INFO     | models.temporal_integrators:import_model:58 - model moved onto: [33mcuda.[0m
2023-05-01 12:57:17.035 | INFO     | models.optimizers:initialize_optimizer:44 - Optimizer:
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 3.527163513710268e-05
    maximize: False
    weight_decay: 0.00030000000000000003
)
2023-05-01 12:57:17.036 | INFO     | datasets.data_processing:lmdb_dataloaders:293 - loading data... 
2023-05-01 12:57:17.036 | INFO     | datasets.data_processing:lmdb_dataloaders:295 - If this process takes long, consider setting is_load_onto_memory=False or use LMDBIterableDataset.
2023-05-01 12:57:19.482 | INFO     | __main__:objective:357 - Starting epoch #0.
2023-05-01 12:57:29.365 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 12:57:29.365 | INFO     | utils.logging:log_training_results:281 - Global Step =      0
2023-05-01 12:57:29.366 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.33876335620880127
2023-05-01 12:57:29.366 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 3.7025651931762695
2023-05-01 12:57:29.367 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.33311769366264343
2023-05-01 12:57:29.367 | INFO     | utils.logging:log_training_results:301 - val MCE loss:1.10198 * 1.0 * 0.31955
2023-05-01 12:57:29.368 | INFO     | utils.logging:log_training_results:309 - val LLLR :1.86481 * 0.0 * 0.55897
2023-05-01 12:57:29.368 | INFO     | utils.logging:log_training_results:317 - val weight decay:1953.15430 * 0.00030000000000000003 * 1.63489
2023-05-01 12:57:30.978 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 12:57:31.478 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 12:57:31.479 | INFO     | utils.logging:log_training_results:281 - Global Step =      0
2023-05-01 12:57:31.480 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.3555890917778015
2023-05-01 12:57:31.480 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 3.66733980178833
2023-05-01 12:57:31.480 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.34276607632637024
2023-05-01 12:57:31.481 | INFO     | utils.logging:log_training_results:301 - train MCE loss:1.10011 * 1.0 * 0.31958
2023-05-01 12:57:31.481 | INFO     | utils.logging:log_training_results:309 - train LLLR :1.78898 * 0.0 * 0.55893
2023-05-01 12:57:31.482 | INFO     | utils.logging:log_training_results:317 - train weight decay:1953.15430 * 0.00030000000000000003 * 1.63485
2023-05-01 12:58:49.227 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 12:58:49.228 | INFO     | utils.logging:log_training_results:281 - Global Step =    250
2023-05-01 12:58:49.229 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.6918267011642456
2023-05-01 12:58:49.229 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 2.8356029987335205
2023-05-01 12:58:49.229 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.7452495694160461
2023-05-01 12:58:49.230 | INFO     | utils.logging:log_training_results:301 - val MCE loss:1.00551 * 1.0 * 0.32804
2023-05-01 12:58:49.230 | INFO     | utils.logging:log_training_results:309 - val LLLR :0.71183 * 0.0 * 0.55012
2023-05-01 12:58:49.231 | INFO     | utils.logging:log_training_results:317 - val weight decay:1941.12341 * 0.00030000000000000003 * 1.62607
2023-05-01 12:58:50.110 | INFO     | utils.checkpoint:update_and_save_result:168 - [36mBest value updated![0m
2023-05-01 12:58:50.133 | INFO     | utils.checkpoint:save_checkpoint:83 - [36mSaved checkpoint[0m at step 250.
2023-05-01 12:58:51.003 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 12:58:51.543 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 12:58:51.544 | INFO     | utils.logging:log_training_results:281 - Global Step =    250
2023-05-01 12:58:51.544 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.6053653955459595
2023-05-01 12:58:51.544 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 2.8676719665527344
2023-05-01 12:58:51.545 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.6633211970329285
2023-05-01 12:58:51.545 | INFO     | utils.logging:log_training_results:301 - train MCE loss:1.01811 * 1.0 * 0.32808
2023-05-01 12:58:51.545 | INFO     | utils.logging:log_training_results:309 - train LLLR :0.85405 * 0.0 * 0.55009
2023-05-01 12:58:51.546 | INFO     | utils.logging:log_training_results:317 - train weight decay:1941.12341 * 0.00030000000000000003 * 1.62603
2023-05-01 12:59:33.628 | INFO     | __main__:objective:357 - Starting epoch #1.
2023-05-01 13:00:10.616 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:00:10.617 | INFO     | utils.logging:log_training_results:281 - Global Step =    500
2023-05-01 13:00:10.617 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.8494366407394409
2023-05-01 13:00:10.618 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 1.1889039278030396
2023-05-01 13:00:10.618 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.9604177474975586
2023-05-01 13:00:10.618 | INFO     | utils.logging:log_training_results:301 - val MCE loss:0.89576 * 1.0 * 0.33557
2023-05-01 13:00:10.619 | INFO     | utils.logging:log_training_results:309 - val LLLR :0.35642 * 0.0 * 0.54121
2023-05-01 13:00:10.619 | INFO     | utils.logging:log_training_results:317 - val weight decay:1935.59412 * 0.00030000000000000003 * 1.61721
2023-05-01 13:00:11.530 | INFO     | utils.checkpoint:update_and_save_result:168 - [36mBest value updated![0m
2023-05-01 13:00:11.552 | INFO     | utils.checkpoint:save_checkpoint:83 - [36mSaved checkpoint[0m at step 500.
2023-05-01 13:00:11.556 | INFO     | utils.checkpoint:keep_max_num_saved_models:101 - Removed the oldest model for max_to_keep=1
2023-05-01 13:00:12.633 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 13:00:13.144 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:00:13.145 | INFO     | utils.logging:log_training_results:281 - Global Step =    500
2023-05-01 13:00:13.145 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.7528671026229858
2023-05-01 13:00:13.146 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 1.9264377355575562
2023-05-01 13:00:13.146 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.860220193862915
2023-05-01 13:00:13.146 | INFO     | utils.logging:log_training_results:301 - train MCE loss:0.92627 * 1.0 * 0.33559
2023-05-01 13:00:13.147 | INFO     | utils.logging:log_training_results:309 - train LLLR :0.63402 * 0.0 * 0.54117
2023-05-01 13:00:13.147 | INFO     | utils.logging:log_training_results:317 - train weight decay:1935.59399 * 0.00030000000000000003 * 1.61718
2023-05-01 13:01:28.697 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:01:28.698 | INFO     | utils.logging:log_training_results:281 - Global Step =    750
2023-05-01 13:01:28.698 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.8626834154129028
2023-05-01 13:01:28.698 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 0.7352371215820312
2023-05-01 13:01:28.699 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.9693971872329712
2023-05-01 13:01:28.699 | INFO     | utils.logging:log_training_results:301 - val MCE loss:0.87762 * 1.0 * 0.34274
2023-05-01 13:01:28.700 | INFO     | utils.logging:log_training_results:309 - val LLLR :0.32552 * 0.0 * 0.53223
2023-05-01 13:01:28.700 | INFO     | utils.logging:log_training_results:317 - val weight decay:1928.93909 * 0.00030000000000000003 * 1.60836
2023-05-01 13:01:29.629 | INFO     | utils.checkpoint:update_and_save_result:168 - [36mBest value updated![0m
2023-05-01 13:01:29.651 | INFO     | utils.checkpoint:save_checkpoint:83 - [36mSaved checkpoint[0m at step 750.
2023-05-01 13:01:29.655 | INFO     | utils.checkpoint:keep_max_num_saved_models:101 - Removed the oldest model for max_to_keep=1
2023-05-01 13:01:30.479 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 13:01:31.001 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:01:31.001 | INFO     | utils.logging:log_training_results:281 - Global Step =    750
2023-05-01 13:01:31.002 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.7740904092788696
2023-05-01 13:01:31.002 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 1.7221850156784058
2023-05-01 13:01:31.002 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.885064959526062
2023-05-01 13:01:31.003 | INFO     | utils.logging:log_training_results:301 - train MCE loss:0.90092 * 1.0 * 0.34277
2023-05-01 13:01:31.003 | INFO     | utils.logging:log_training_results:309 - train LLLR :0.58065 * 0.0 * 0.53220
2023-05-01 13:01:31.004 | INFO     | utils.logging:log_training_results:317 - train weight decay:1928.93909 * 0.00030000000000000003 * 1.60832
2023-05-01 13:01:45.118 | INFO     | __main__:objective:357 - Starting epoch #2.
2023-05-01 13:02:51.255 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:02:51.256 | INFO     | utils.logging:log_training_results:281 - Global Step =   1000
2023-05-01 13:02:51.256 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.8603566288948059
2023-05-01 13:02:51.257 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 0.7563561797142029
2023-05-01 13:02:51.257 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.9667868614196777
2023-05-01 13:02:51.258 | INFO     | utils.logging:log_training_results:301 - val MCE loss:0.87550 * 1.0 * 0.34983
2023-05-01 13:02:51.258 | INFO     | utils.logging:log_training_results:309 - val LLLR :0.33345 * 0.0 * 0.52320
2023-05-01 13:02:51.259 | INFO     | utils.logging:log_training_results:317 - val weight decay:1922.30994 * 0.00030000000000000003 * 1.59948
2023-05-01 13:02:53.014 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 13:02:53.550 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:02:53.550 | INFO     | utils.logging:log_training_results:281 - Global Step =   1000
2023-05-01 13:02:53.551 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.7515555620193481
2023-05-01 13:02:53.551 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 1.7036075592041016
2023-05-01 13:02:53.551 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.8703415989875793
2023-05-01 13:02:53.552 | INFO     | utils.logging:log_training_results:301 - train MCE loss:0.89626 * 1.0 * 0.34986
2023-05-01 13:02:53.552 | INFO     | utils.logging:log_training_results:309 - train LLLR :0.68328 * 0.0 * 0.52317
2023-05-01 13:02:53.553 | INFO     | utils.logging:log_training_results:317 - train weight decay:1922.30994 * 0.00030000000000000003 * 1.59944
2023-05-01 13:03:49.987 | INFO     | __main__:objective:357 - Starting epoch #3.
2023-05-01 13:04:13.780 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:04:13.781 | INFO     | utils.logging:log_training_results:281 - Global Step =   1250
2023-05-01 13:04:13.782 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.866836667060852
2023-05-01 13:04:13.782 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 0.5785188674926758
2023-05-01 13:04:13.783 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.9723448753356934
2023-05-01 13:04:13.783 | INFO     | utils.logging:log_training_results:301 - val MCE loss:0.87061 * 1.0 * 0.35684
2023-05-01 13:04:13.784 | INFO     | utils.logging:log_training_results:309 - val LLLR :0.31514 * 0.0 * 0.51411
2023-05-01 13:04:13.784 | INFO     | utils.logging:log_training_results:317 - val weight decay:1915.84851 * 0.00030000000000000003 * 1.59059
2023-05-01 13:04:14.714 | INFO     | utils.checkpoint:update_and_save_result:168 - [36mBest value updated![0m
2023-05-01 13:04:14.739 | INFO     | utils.checkpoint:save_checkpoint:83 - [36mSaved checkpoint[0m at step 1250.
2023-05-01 13:04:14.742 | INFO     | utils.checkpoint:keep_max_num_saved_models:101 - Removed the oldest model for max_to_keep=1
2023-05-01 13:04:15.570 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 13:04:16.106 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:04:16.106 | INFO     | utils.logging:log_training_results:281 - Global Step =   1250
2023-05-01 13:04:16.106 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.7816073298454285
2023-05-01 13:04:16.107 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 1.5596455335617065
2023-05-01 13:04:16.107 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.9148117899894714
2023-05-01 13:04:16.108 | INFO     | utils.logging:log_training_results:301 - train MCE loss:0.91100 * 1.0 * 0.35687
2023-05-01 13:04:16.108 | INFO     | utils.logging:log_training_results:309 - train LLLR :0.57247 * 0.0 * 0.51408
2023-05-01 13:04:16.108 | INFO     | utils.logging:log_training_results:317 - train weight decay:1915.84851 * 0.00030000000000000003 * 1.59056
2023-05-01 13:05:34.412 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:05:34.413 | INFO     | utils.logging:log_training_results:281 - Global Step =   1500
2023-05-01 13:05:34.414 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.8682466745376587
2023-05-01 13:05:34.414 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 0.5289161205291748
2023-05-01 13:05:34.414 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.9739543795585632
2023-05-01 13:05:34.415 | INFO     | utils.logging:log_training_results:301 - val MCE loss:0.86921 * 1.0 * 0.36376
2023-05-01 13:05:34.415 | INFO     | utils.logging:log_training_results:309 - val LLLR :0.31180 * 0.0 * 0.50498
2023-05-01 13:05:34.416 | INFO     | utils.logging:log_training_results:317 - val weight decay:1909.48560 * 0.00030000000000000003 * 1.58168
2023-05-01 13:05:35.131 | INFO     | utils.checkpoint:update_and_save_result:168 - [36mBest value updated![0m
2023-05-01 13:05:35.153 | INFO     | utils.checkpoint:save_checkpoint:83 - [36mSaved checkpoint[0m at step 1500.
2023-05-01 13:05:35.156 | INFO     | utils.checkpoint:keep_max_num_saved_models:101 - Removed the oldest model for max_to_keep=1
2023-05-01 13:05:35.976 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 13:05:36.508 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:05:36.508 | INFO     | utils.logging:log_training_results:281 - Global Step =   1500
2023-05-01 13:05:36.509 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.8117972612380981
2023-05-01 13:05:36.509 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 1.5815924406051636
2023-05-01 13:05:36.509 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.9127906560897827
2023-05-01 13:05:36.510 | INFO     | utils.logging:log_training_results:301 - train MCE loss:0.90916 * 1.0 * 0.36378
2023-05-01 13:05:36.510 | INFO     | utils.logging:log_training_results:309 - train LLLR :0.49271 * 0.0 * 0.50494
2023-05-01 13:05:36.511 | INFO     | utils.logging:log_training_results:317 - train weight decay:1909.48560 * 0.00030000000000000003 * 1.58165
2023-05-01 13:06:04.435 | INFO     | __main__:objective:357 - Starting epoch #4.
2023-05-01 13:06:55.499 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:06:55.500 | INFO     | utils.logging:log_training_results:281 - Global Step =   1750
2023-05-01 13:06:55.501 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.8674033284187317
2023-05-01 13:06:55.501 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 0.554806113243103
2023-05-01 13:06:55.502 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.9698964357376099
2023-05-01 13:06:55.502 | INFO     | utils.logging:log_training_results:301 - val MCE loss:0.86760 * 1.0 * 0.37059
2023-05-01 13:06:55.503 | INFO     | utils.logging:log_training_results:309 - val LLLR :0.31543 * 0.0 * 0.49579
2023-05-01 13:06:55.503 | INFO     | utils.logging:log_training_results:317 - val weight decay:1903.01453 * 0.00030000000000000003 * 1.57277
2023-05-01 13:06:57.125 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 13:06:57.644 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:06:57.644 | INFO     | utils.logging:log_training_results:281 - Global Step =   1750
2023-05-01 13:06:57.645 | INFO     | utils.logging:log_training_results:282 - train mean(MacRec)_t: 0.8110891580581665
2023-05-01 13:06:57.645 | INFO     | utils.logging:log_training_results:287 - train mean ABSerr: 1.5689464807510376
2023-05-01 13:06:57.646 | INFO     | utils.logging:log_training_results:290 - train ausat from confmx: 0.9116442799568176
2023-05-01 13:06:57.646 | INFO     | utils.logging:log_training_results:301 - train MCE loss:0.87266 * 1.0 * 0.37062
2023-05-01 13:06:57.646 | INFO     | utils.logging:log_training_results:309 - train LLLR :0.52253 * 0.0 * 0.49576
2023-05-01 13:06:57.647 | INFO     | utils.logging:log_training_results:317 - train weight decay:1903.01440 * 0.00030000000000000003 * 1.57274
2023-05-01 13:08:07.115 | INFO     | __main__:objective:357 - Starting epoch #5.
2023-05-01 13:08:16.795 | INFO     | utils.logging:log_training_results:280 - 
2023-05-01 13:08:16.795 | INFO     | utils.logging:log_training_results:281 - Global Step =   2000
2023-05-01 13:08:16.796 | INFO     | utils.logging:log_training_results:282 - val mean(MacRec)_t: 0.864103376865387
2023-05-01 13:08:16.796 | INFO     | utils.logging:log_training_results:287 - val mean ABSerr: 0.6431354284286499
2023-05-01 13:08:16.797 | INFO     | utils.logging:log_training_results:290 - val ausat from confmx: 0.9682905077934265
2023-05-01 13:08:16.797 | INFO     | utils.logging:log_training_results:301 - val MCE loss:0.86769 * 1.0 * 0.37733
2023-05-01 13:08:16.798 | INFO     | utils.logging:log_training_results:309 - val LLLR :0.32379 * 0.0 * 0.48657
2023-05-01 13:08:16.798 | INFO     | utils.logging:log_training_results:317 - val weight decay:1896.48474 * 0.00030000000000000003 * 1.56385
2023-05-01 13:08:18.470 | INFO     | utils.logging:save_sdre_imgs:710 - Figures saved.
2023-05-01 13:08:19.636 | ERROR    | __main__:run_one_epoch:282 - An error has been caught in function 'run_one_epoch', process 'MainProcess' (3590), thread 'MainThread' (140214706634752):
Traceback (most recent call last):

  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
           â”‚         â”‚     â”” {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': <_frozen_importlib_external.SourceFileLoader objec...
           â”‚         â”” <code object <module> at 0x7f8647a66b80, file "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/...
           â”” <function _run_code at 0x7f8647aafac0>
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
         â”‚     â”” {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': <_frozen_importlib_external.SourceFileLoader objec...
         â”” <code object <module> at 0x7f8647a66b80, file "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/...

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py", line 39, in <module>
    cli.main()
    â”‚   â”” <function main at 0x7f864669ab90>
    â”” <module 'debugpy.server.cli' from '/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debug...

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 430, in main
    run()
    â”” <function run_file at 0x7f864669a950>

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 284, in run_file
    runpy.run_path(target, run_name="__main__")
    â”‚     â”‚        â”” '/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py'
    â”‚     â”” <function run_path at 0x7f8646a711b0>
    â”” <module '_pydevd_bundle.pydevd_runpy' from '/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/pyt...

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
           â”‚                â”‚     â”‚             â”” '__main__'
           â”‚                â”‚     â”” None
           â”‚                â”” <code object <module> at 0x7f86466c3730, file "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 1>
           â”” <function _run_module_code at 0x7f8646a70e50>

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
    â”‚         â”‚     â”‚            â”” None
    â”‚         â”‚     â”” {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': None, '__spec__': None, '__file__': '/home/afe/Dro...
    â”‚         â”” <code object <module> at 0x7f86466c3730, file "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 1>
    â”” <function _run_code at 0x7f8646a70a60>

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 124, in _run_code
    exec(code, run_globals)
         â”‚     â”” {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': None, '__spec__': None, '__file__': '/home/afe/Dro...
         â”” <code object <module> at 0x7f86466c3730, file "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 1>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 391, in <module>
    main()
    â”” <function main at 0x7f847931bb50>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 385, in main
    run_optuna(objective, device, config_orig)
    â”‚          â”‚          â”‚       â”” {'VERBOSE': True, 'CONFIG_PATH': '/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/config/config_definition.py', 'IS_SEED': False...
    â”‚          â”‚          â”” device(type='cuda')
    â”‚          â”” <function objective at 0x7f847931bac0>
    â”” <function run_optuna at 0x7f84794a8c10>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/utils/hyperparameter_tuning.py", line 334, in run_optuna
    else start_optimization(objective, conf)
         â”‚                  â”‚          â”” <utils.misc.ConfigSubset object at 0x7f8479307e20>
         â”‚                  â”” <function objective at 0x7f847931bac0>
         â”” <function run_optuna.<locals>.start_optimization at 0x7f847931bd00>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/utils/hyperparameter_tuning.py", line 208, in start_optimization
    study.optimize(
    â”‚     â”” <function Study.optimize at 0x7f847dc64b80>
    â”” <optuna.study.study.Study object at 0x7f8471d02ef0>

  File "/usr/local/lib/python3.10/dist-packages/optuna/study/study.py", line 425, in optimize
    _optimize(
    â”” <function _optimize at 0x7f847de3ef80>
  File "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
    â”” <function _optimize_sequential at 0x7f847dc64310>
  File "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
                   â”‚          â”‚      â”‚     â”” ()
                   â”‚          â”‚      â”” <function run_optuna.<locals>.start_optimization.<locals>.<lambda> at 0x7f8471cb0b80>
                   â”‚          â”” <optuna.study.study.Study object at 0x7f8471d02ef0>
                   â”” <function _run_trial at 0x7f847dc643a0>
  File "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
                      â”‚    â”” <optuna.trial._trial.Trial object at 0x7f8616ebe560>
                      â”” <function run_optuna.<locals>.start_optimization.<locals>.<lambda> at 0x7f8471cb0b80>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/utils/hyperparameter_tuning.py", line 209, in <lambda>
    lambda trial: objective(trial, device, config),
           â”‚      â”‚         â”‚      â”‚       â”” {'VERBOSE': True, 'CONFIG_PATH': '/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/config/config_definition.py', 'IS_SEED': False...
           â”‚      â”‚         â”‚      â”” device(type='cuda')
           â”‚      â”‚         â”” <optuna.trial._trial.Trial object at 0x7f8616ebe560>
           â”‚      â”” <function objective at 0x7f847931bac0>
           â”” <optuna.trial._trial.Trial object at 0x7f8616ebe560>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 358, in objective
    best, global_step = run_one_epoch(
    â”‚                   â”” <function run_one_epoch at 0x7f847931ba30>
    â”” (0.1317533254623413,)

> File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 282, in run_one_epoch
    ) = iterating_over_dataset(
        â”” <function iterating_over_dataset at 0x7f847931b400>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 140, in iterating_over_dataset
    best, global_step = run_one_epoch(
    â”‚                   â”” <function run_one_epoch at 0x7f847931ba30>
    â”” (0.1317533254623413,)

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 309, in run_one_epoch
    report_to_pruner(trial, best, global_step, config)
    â”‚                â”‚      â”‚     â”‚            â”” {'VERBOSE': True, 'CONFIG_PATH': '/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/config/config_definition.py', 'IS_SEED': False...
    â”‚                â”‚      â”‚     â”” 2000
    â”‚                â”‚      â”” (0.1317533254623413,)
    â”‚                â”” <optuna.trial._trial.Trial object at 0x7f8616ebe560>
    â”” <function report_to_pruner at 0x7f84794a9510>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/utils/hyperparameter_tuning.py", line 607, in report_to_pruner
    raise optuna.TrialPruned()
          â”‚      â”” <class 'optuna.exceptions.TrialPruned'>
          â”” <module 'optuna' from '/usr/local/lib/python3.10/dist-packages/optuna/__init__.py'>

optuna.exceptions.TrialPruned
2023-05-01 13:08:19.666 | INFO     | utils.logging:investigate_log:155 - num_warning=0/num_error=0 in the .log file.
2023-05-01 13:08:19.856 | WARNING  | optuna.study._optimize:_log_failed_trial:261 - Trial 107 failed with parameters: {'LIST_IS_POSITIONAL_ENCODING': True, 'LIST_IS_TRAINABLE_ENCODING': True, 'LIST_MODEL_BACKBONE': 'Transformer', 'LIST_WEIGHT_DECAY': 0.00030000000000000003, 'LIST_LEARNING_RATE': 3.527163513710268e-05, 'LIST_LLLR_VERSION': 'LSEL', 'LIST_ACTIVATION_FC': 'B2Bcbrt', 'LIST_PARAM_MULTIPLET_LOSS': 1.0, 'LIST_PARAM_LLR_LOSS': 0.0, 'LIST_IS_ADAPTIVE_LOSS': True, 'LIST_ORDER_SPRT': 5, 'LIST_OPTIMIZER': 'adam', 'LIST_IS_NORMALIZE': False, 'LIST_NUM_THRESH': 500, 'LIST_SPARSITY': 'logspace', 'LIST_NUM_BLOCKS': 2, 'LIST_NUM_HEADS': 2, 'LIST_DROPOUT': 0.30000000000000004, 'LIST_FF_DIM': 64, 'LIST_MLP_UNITS': 64} because of the following error: TypeError('cannot unpack non-iterable NoneType object').
Traceback (most recent call last):

  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
           â”‚         â”‚     â”” {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': <_frozen_importlib_external.SourceFileLoader objec...
           â”‚         â”” <code object <module> at 0x7f8647a66b80, file "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/...
           â”” <function _run_code at 0x7f8647aafac0>
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
         â”‚     â”” {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': <_frozen_importlib_external.SourceFileLoader objec...
         â”” <code object <module> at 0x7f8647a66b80, file "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/...

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py", line 39, in <module>
    cli.main()
    â”‚   â”” <function main at 0x7f864669ab90>
    â”” <module 'debugpy.server.cli' from '/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debug...

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 430, in main
    run()
    â”” <function run_file at 0x7f864669a950>

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 284, in run_file
    runpy.run_path(target, run_name="__main__")
    â”‚     â”‚        â”” '/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py'
    â”‚     â”” <function run_path at 0x7f8646a711b0>
    â”” <module '_pydevd_bundle.pydevd_runpy' from '/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/pyt...

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
           â”‚                â”‚     â”‚             â”” '__main__'
           â”‚                â”‚     â”” None
           â”‚                â”” <code object <module> at 0x7f86466c3730, file "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 1>
           â”” <function _run_module_code at 0x7f8646a70e50>

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
    â”‚         â”‚     â”‚            â”” None
    â”‚         â”‚     â”” {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': None, '__spec__': None, '__file__': '/home/afe/Dro...
    â”‚         â”” <code object <module> at 0x7f86466c3730, file "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 1>
    â”” <function _run_code at 0x7f8646a70a60>

  File "/home/afe/.vscode-server/extensions/ms-python.python-2023.6.1/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 124, in _run_code
    exec(code, run_globals)
         â”‚     â”” {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': None, '__spec__': None, '__file__': '/home/afe/Dro...
         â”” <code object <module> at 0x7f86466c3730, file "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 1>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 391, in <module>
    main()
    â”” <function main at 0x7f847931bb50>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 385, in main
    run_optuna(objective, device, config_orig)
    â”‚          â”‚          â”‚       â”” {'VERBOSE': True, 'CONFIG_PATH': '/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/config/config_definition.py', 'IS_SEED': False...
    â”‚          â”‚          â”” device(type='cuda')
    â”‚          â”” <function objective at 0x7f847931bac0>
    â”” <function run_optuna at 0x7f84794a8c10>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/utils/hyperparameter_tuning.py", line 334, in run_optuna
    else start_optimization(objective, conf)
         â”‚                  â”‚          â”” <utils.misc.ConfigSubset object at 0x7f8479307e20>
         â”‚                  â”” <function objective at 0x7f847931bac0>
         â”” <function run_optuna.<locals>.start_optimization at 0x7f847931bd00>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/utils/hyperparameter_tuning.py", line 208, in start_optimization
    study.optimize(
    â”‚     â”” <function Study.optimize at 0x7f847dc64b80>
    â”” <optuna.study.study.Study object at 0x7f8471d02ef0>

  File "/usr/local/lib/python3.10/dist-packages/optuna/study/study.py", line 425, in optimize
    _optimize(
    â”” <function _optimize at 0x7f847de3ef80>
  File "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
    â”” <function _optimize_sequential at 0x7f847dc64310>
  File "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
                   â”‚          â”‚      â”‚     â”” ()
                   â”‚          â”‚      â”” <function run_optuna.<locals>.start_optimization.<locals>.<lambda> at 0x7f8471cb0b80>
                   â”‚          â”” <optuna.study.study.Study object at 0x7f8471d02ef0>
                   â”” <function _run_trial at 0x7f847dc643a0>
> File "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
                      â”‚    â”” <optuna.trial._trial.Trial object at 0x7f8616ebe560>
                      â”” <function run_optuna.<locals>.start_optimization.<locals>.<lambda> at 0x7f8471cb0b80>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/utils/hyperparameter_tuning.py", line 209, in <lambda>
    lambda trial: objective(trial, device, config),
           â”‚      â”‚         â”‚      â”‚       â”” {'VERBOSE': True, 'CONFIG_PATH': '/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/config/config_definition.py', 'IS_SEED': False...
           â”‚      â”‚         â”‚      â”” device(type='cuda')
           â”‚      â”‚         â”” <optuna.trial._trial.Trial object at 0x7f8616ebe560>
           â”‚      â”” <function objective at 0x7f847931bac0>
           â”” <optuna.trial._trial.Trial object at 0x7f8616ebe560>

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 358, in objective
    best, global_step = run_one_epoch(
    â”‚                   â”” <function run_one_epoch at 0x7f847931ba30>
    â”” (0.1317533254623413,)

  File "/home/afe/Dropbox/GitHub/SPRT-TANDEM-PyTorch/sprt_tandem_main.py", line 277, in run_one_epoch
    (

TypeError: cannot unpack non-iterable NoneType object
2023-05-01 13:08:19.869 | WARNING  | optuna.study._optimize:_log_failed_trial:268 - Trial 107 failed with value None.
2023-05-01 13:08:20.236 | INFO     | utils.misc:__exit__:207 - elapsed time:  8.518824 hours. ()
2023-05-01 13:08:20.261 | INFO     | utils.misc:__exit__:208 - Memory usage: 1960.11 megabytes
